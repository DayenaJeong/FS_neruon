# swish_h = [0.7844085, 1.7550646, 1.4212719, 1.9244663,
           1.4400164, 1.9249276, 1.6100428, 1.29061, 0.83985287,
           3.0827985, 0.43503317, 0.5155389, 0.3269445, 7.618048,
           -14.419675, 0]
# swish_d = [0.40088242, 1.8856316, 1.4915457, 1.947229, 1.4717816, 1.9147849,
           1.6389303, 1.2934641, 0.8703124, 0.68460804, 0.44262582, 0.32832766,
           0.13907616, -0.26217145, 0.23736191, -0.14830568]
# swish_T = [0.05946945, 1.6541206, 1.3066754, 1.3267198, 1.1000695, 1.2308974,
           1.1188028, 0.9472501, 0.5447531, 1.0236325, 0.11921431, -0.12855051,
           -0.44731247, -2.1383545, 1., -4.109145]

import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd

# PyTorch의 사용자 정의 그래디언트를 위한 spike function 구현
class SpikeFunction(autograd.Function):
    @staticmethod
    def forward(ctx, v_scaled):
        z_ = torch.where(v_scaled > 0, torch.ones_like(v_scaled), torch.zeros_like(v_scaled))
        ctx.save_for_backward(v_scaled)
        return z_

    @staticmethod
    def backward(ctx, grad_output):
        v_scaled, = ctx.saved_tensors
        dz_dv_scaled = torch.maximum(1 - torch.abs(v_scaled), torch.tensor(0.0, device=v_scaled.device))
        dE_dv_scaled = grad_output * dz_dv_scaled
        return dE_dv_scaled

# PyTorch spike function 호출
def spike_function(v_scaled):
    return SpikeFunction.apply(v_scaled)

# AMOSUnit 클래스 정의
class AMOSUnit(nn.Module):
    def __init__(self, input_dim, k_neurons, swish_h, swish_d, swish_T):
        super(AMOSUnit, self).__init__()
        self.k_neurons = k_neurons
        
        # FS 파라미터 정의
        self.h = torch.tensor(swish_h, dtype=torch.float32, requires_grad=False)
        self.d = torch.tensor(swish_d, dtype=torch.float32, requires_grad=False)
        self.T = torch.tensor(swish_T, dtype=torch.float32, requires_grad=False)
        
        # 학습 가능한 매개변수 추가
        self.linear = nn.Linear(input_dim, k_neurons)
        
        # 최종 출력을 위한 추가 선형 레이어
        self.final_linear = nn.Linear(k_neurons, 1)

    def forward(self, x):
        # 선형 변환 적용
        v = self.linear(x)
        
        # FS 스파이크 신경망을 위한 임시 출력 초기화
        temp_out = torch.zeros_like(v)
        
        # FS 스파이크 신경망 구현
        for t in range(len(self.T)):
            v_scaled = (v - self.T[t]) / (torch.abs(v) + 1)
            z = spike_function(v_scaled)
            temp_out += z * self.d[t]
            v = v - z * self.h[t]
        
        # 최종 출력을 위해 추가된 선형 레이어 적용
        out = self.final_linear(temp_out)
        
        return out

# 모델 인스턴스화
input_dim = 1
k_neurons = len(swish_h)  # swish_h 길이에 맞춰 k_neurons 설정
model = AMOSUnit(input_dim, k_neurons, swish_h, swish_d, swish_T)

# 손실 함수 및 옵티마이저
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 훈련 데이터
x_train = torch.linspace(-5, 5, steps=100).unsqueeze(1)  # 입력
y_train = swish_function(x_train)  # 목표값 (실제 Swish 함수 값)
y_train_expanded = y_train.expand(-1, y_pred.size(1))

torch.autograd.set_detect_anomaly(True)

# 훈련 루프
epochs = 10000
for epoch in range(epochs):
    optimizer.zero_grad()  # 그래디언트 초기화
    y_pred = model(x_train)  # 순전파
    loss = criterion(y_pred, y_train)  # `.squeeze()` 호출 제거
    loss.backward()  # 역전파
    optimizer.step()  # 가중치 업데이트

    # 100번마다 손실 출력
    if epoch % 100 == 0:
       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')

# 최종 손실 출력
print(f'Final Loss: {loss.item()}')
