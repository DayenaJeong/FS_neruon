{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1ax3ACpN2IIH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yBp7GfIK2L4c"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "\n",
        "def softplus(x, beta=1):\n",
        "    return (1 / beta) * np.log(1 + np.exp(beta * x))\n",
        "\n",
        "\n",
        "def mish(x, beta=1):\n",
        "    return x * np.tanh(softplus(x, beta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ybDTMeqS2Oq2"
      },
      "outputs": [],
      "source": [
        "# Implementation of spike function for PyTorch custom gradient\n",
        "class SpikeFunction(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, v_scaled):\n",
        "        z_ = torch.where(v_scaled > 0, torch.ones_like(v_scaled), torch.zeros_like(v_scaled))\n",
        "        ctx.save_for_backward(v_scaled)\n",
        "        return z_\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        v_scaled, = ctx.saved_tensors\n",
        "        dz_dv_scaled = torch.maximum(1 - torch.abs(v_scaled), torch.tensor(0.0, device=v_scaled.device))\n",
        "        dE_dv_scaled = grad_output * dz_dv_scaled\n",
        "        return dE_dv_scaled\n",
        "\n",
        "# Call spike function for PyTorch\n",
        "def spike_function(v_scaled):\n",
        "    return SpikeFunction.apply(v_scaled)\n",
        "\n",
        "# FS class definition\n",
        "class FS(nn.Module):\n",
        "    def __init__(self, num_params):\n",
        "        super(FS, self).__init__()\n",
        "\n",
        "        super(FS, self).__init__()\n",
        "        self.num_params = num_params\n",
        "        self.indices = torch.arange(1, num_params + 1).float()\n",
        "\n",
        "        if num_params == 4:\n",
        "            h = torch.tensor([6.0247, 3.0090, 1.5470, 1.0945])\n",
        "            d = torch.tensor([6.1718, 3.0962, 1.5492, 0.7664])\n",
        "            T = torch.tensor([5.8070, 2.8584, 1.3577, 0.5687])\n",
        "        elif num_params == 8:\n",
        "            h = torch.tensor([3.6760, 2.8168, 2.3811, 1.3095, 0.8152, 0.3878, 0.1751, 2.4152])\n",
        "            d = torch.tensor([2.9805, 2.4332, 1.3228, 0.8048, 0.3861, 0.1863, 0.1023])\n",
        "            T = torch.tensor([8.2595, 3.5255, 2.5539, 1.7761, 0.9528, 0.5461, 0.3573, 0.2881])\n",
        "        elif num_params == 12:\n",
        "            h = torch.tensor([1.1299, 6.8735, 4.4682, 2.0329, 1.3388, 0.7357, 0.7149, 0.4261, 0.6461, 0.4154, 0.2179, 0.3444])\n",
        "            d = torch.tensor([-0.2083,  7.0264,  4.6516,  2.1096,  1.3740,  0.7480,  0.6828,  0.3809, 0.4077,  0.2240,  0.1159,  0.0536])\n",
        "            T = torch.tensor([-3.5203,  5.9076,  4.0212,  1.8142,  0.9658,  0.3212, -0.0740, -0.4722, -0.7314, -1.2238, -1.3265, -1.4648])\n",
        "        elif num_params == 16:\n",
        "            h = torch.tensor([0.5602, 0.3329, 1.2282, 1.4916, 1.0170, 1.0777, 0.9202, 1.2722, 2.8685, 1.3212, 0.8411, 0.3989, 0.1870, 0.0936, 0.0616, 1.1349])\n",
        "            d = torch.tensor([0.3161,  0.2175, -0.2093,  1.6195,  1.0840,  1.1121,  0.9249,  1.2900, 2.8708,  1.3277,  0.8375,  0.3912,  0.1875,  0.0973,  0.0640,  0.0406])\n",
        "            T = torch.tensor([0.0984, -0.0835, -3.5130,  0.6933,  0.1460,  0.0137,  0.9415,  0.2020, 1.2894, -0.2560, -0.7142, -1.1954, -1.3832, -1.4649, -1.4901, -1.5134])\n",
        "\n",
        "        # if self.num_params == 4:\n",
        "        #     self.h = 0.6408 * self.indices**2 - 4.8293 * self.indices + 10.186\n",
        "        #     self.d = 0.5732 * self.indices**2 - 4.6423 * self.indices + 10.203\n",
        "        #     self.T = 0.5399 * self.indices**2 - 4.4211 * self.indices + 9.6514\n",
        "        # elif self.num_params == 8:\n",
        "        #     self.h = -1.378 * torch.log(self.indices) + 3.5742\n",
        "        #     self.d = -1.92 * torch.log(self.indices) + 4.0338\n",
        "        #     self.T = -3.663 * torch.log(self.indices) + 7.1379\n",
        "        # elif self.num_params == 12:\n",
        "        #     self.h = 0.0289 * self.indices**2 - 0.7486 * self.indices + 4.9117\n",
        "        #     self.d = 0.0078 * self.indices**2 - 0.4573 * self.indices + 4.0149\n",
        "        #     self.T = -0.0495 * self.indices**2 + 0.3057 * self.indices + 1.0462\n",
        "        # elif self.num_params == 16:\n",
        "        #     self.h = -0.018 * self.indices**2 + 0.2526 * self.indices + 0.3753\n",
        "        #     self.d = -0.0284 * self.indices**2 + 0.3955 * self.indices + 0.0063\n",
        "        #     self.T = -0.021 * self.indices**2 + 0.1795 * self.indices - 0.0105\n",
        "\n",
        "        self.h = nn.Parameter(h + torch.normal(0, 0.1, size=h.size()))\n",
        "        self.d = nn.Parameter(d + torch.normal(0, 0.1, size=d.size()))\n",
        "        self.T = nn.Parameter(T + torch.normal(0, 0.1, size=T.size()))\n",
        "\n",
        "        # self.h = nn.Parameter(h)\n",
        "        # self.d = nn.Parameter(d)\n",
        "        # self.T = nn.Parameter(T)\n",
        "\n",
        "        # self.h = nn.Parameter(self.h)\n",
        "        # self.d = nn.Parameter(self.d)\n",
        "        # self.T = nn.Parameter(self.T)\n",
        "\n",
        "        #Define FS parameters (now as learnable parameters)\n",
        "        #self.h = nn.Parameter(torch.abs(torch.randn(num_params)))\n",
        "        #self.d = nn.Parameter(torch.abs(torch.randn(num_params)))\n",
        "        #self.T = nn.Parameter(torch.randn(num_params))\n",
        "\n",
        "    def forward(self, x):\n",
        "        v = x.clone()\n",
        "\n",
        "        # Initialize temporary output for FS spike neural network\n",
        "        temp_out = torch.zeros_like(v)\n",
        "\n",
        "        noisy_h = self.h + torch.normal(0, 0.1, size=self.h.size(), device=self.h.device)\n",
        "        noisy_d = self.d + torch.normal(0, 0.1, size=self.d.size(), device=self.d.device)\n",
        "        noisy_T = self.T + torch.normal(0, 0.1, size=self.T.size(), device=self.T.device)\n",
        "\n",
        "        # Implement FS spike neural network\n",
        "        for t in range(len(self.T)):\n",
        "            #v_scaled = (v - self.T[t]) / (torch.abs(v) + 1)\n",
        "            v_scaled = v - noisy_T[t]\n",
        "            z = spike_function(v_scaled)\n",
        "            temp_out += z * noisy_d[t]\n",
        "            v = v - z * noisy_h[t]\n",
        "\n",
        "        return temp_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy91fjpVobm1"
      },
      "source": [
        "# Swish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmGykx5E57gn"
      },
      "source": [
        "K=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N47DbIYXodEM",
        "outputId": "e981b2e1-a77b-4657-bd8d-8e864e59a79a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20000, Loss: 0.08101660013198853\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x_train)  \u001b[38;5;66;03m# Forward pass, including v_reg and z_reg calculations\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_train)  \u001b[38;5;66;03m# Remove `.squeeze()` call\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Print loss every 1000 epochs\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Instantiate model and initial setup\n",
        "num_params = 4  # Select arbitrary number of parameters\n",
        "model = FS(num_params)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training data\n",
        "x_train = torch.linspace(-8, 12, steps=100000).unsqueeze(1)\n",
        "\n",
        "# x_train = torch.cat((\n",
        "#     torch.linspace(-8, -5, steps=5000),\n",
        "#     torch.linspace(-5, 0, steps=60000),\n",
        "#     torch.linspace(0, 12, steps=35000)\n",
        "# )).unsqueeze(1)\n",
        "\n",
        "y_train = swish(x_train)  # Target value\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n",
        "    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "       loss_values.append(loss.item())\n",
        "\n",
        "# Print final loss\n",
        "print(\"Loss values:\", loss_values)\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "print(f'Final h: {model.h.data}')\n",
        "print(f'Final d: {model.d.data}')\n",
        "print(f'Final T: {model.T.data}')\n",
        "\n",
        "# Plot loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(0, epochs, 200), loss_values, label='Loss per Epoch')\n",
        "plt.title('Loss Progression over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Calculate model predictions\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    y_pred = model(x_train).squeeze()\n",
        "\n",
        "# True function values\n",
        "y_true = swish(x_train).squeeze()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_train.numpy(), y_true.numpy(), label='Swish', color='r')\n",
        "plt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n",
        "# plt.xlabel('Input x')\n",
        "# plt.ylabel('Output y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rstbv3CY57go"
      },
      "source": [
        "k=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Zd0mv2aF57go",
        "outputId": "365a1014-3499-4144-dae3-36343aecf265"
      },
      "outputs": [],
      "source": [
        "# Instantiate model and initial setup\n",
        "num_params = 8  # Select arbitrary number of parameters\n",
        "model = FS(num_params)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training data\n",
        "x_train = torch.linspace(-8, 12, steps=100000).unsqueeze(1)\n",
        "\n",
        "# x_train = torch.cat((\n",
        "#     torch.linspace(-8, -5, steps=5000),\n",
        "#     torch.linspace(-5, 0, steps=60000),\n",
        "#     torch.linspace(0, 12, steps=35000)\n",
        "# )).unsqueeze(1)\n",
        "\n",
        "y_train = swish(x_train)  # Target value\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n",
        "    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "       loss_values.append(loss.item())\n",
        "\n",
        "# Print final loss\n",
        "print(\"Loss values:\", loss_values)\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "print(f'Final h: {model.h.data}')\n",
        "print(f'Final d: {model.d.data}')\n",
        "print(f'Final T: {model.T.data}')\n",
        "\n",
        "# Calculate model predictions\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    y_pred = model(x_train).squeeze()\n",
        "\n",
        "# True function values\n",
        "y_true = swish(x_train).squeeze()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_train.numpy(), y_true.numpy(), label='True Swish Function', color='r')\n",
        "plt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n",
        "plt.title('Comparison between True Swish Function and Model Prediction')\n",
        "plt.xlabel('Input x')\n",
        "plt.ylabel('Output y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsn2q_w557go"
      },
      "source": [
        "k=12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hmn_Xp0X57go",
        "outputId": "90559b64-433c-473b-b924-123050bf5ac1"
      },
      "outputs": [],
      "source": [
        "# Instantiate model and initial setup\n",
        "num_params = 12  # Select arbitrary number of parameters\n",
        "model = FS(num_params)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training data\n",
        "x_train = torch.linspace(-8, 12, steps=100000).unsqueeze(1)\n",
        "\n",
        "# x_train = torch.cat((\n",
        "#     torch.linspace(-8, -5, steps=5000),\n",
        "#     torch.linspace(-5, 0, steps=60000),\n",
        "#     torch.linspace(0, 12, steps=35000)\n",
        "# )).unsqueeze(1)\n",
        "\n",
        "y_train = swish(x_train)  # Target value\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n",
        "    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "       loss_values.append(loss.item())\n",
        "\n",
        "# Print final loss\n",
        "print(\"Loss values:\", loss_values)\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "print(f'Final h: {model.h.data}')\n",
        "print(f'Final d: {model.d.data}')\n",
        "print(f'Final T: {model.T.data}')\n",
        "\n",
        "# Calculate model predictions\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    y_pred = model(x_train).squeeze()\n",
        "\n",
        "# True function values\n",
        "y_true = swish(x_train).squeeze()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_train.numpy(), y_true.numpy(), label='True Swish Function', color='r')\n",
        "plt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n",
        "plt.title('Comparison between True Swish Function and Model Prediction')\n",
        "plt.xlabel('Input x')\n",
        "plt.ylabel('Output y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7fn0jWj57go"
      },
      "source": [
        "K=16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pZ7UMq7m57go",
        "outputId": "fa31f0f4-4872-4084-b390-5e835fbc35ae"
      },
      "outputs": [],
      "source": [
        "# Instantiate model and initial setup\n",
        "num_params = 16  # Select arbitrary number of parameters\n",
        "model = FS(num_params)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Training data\n",
        "x_train = torch.linspace(-8, 12, steps=100000).unsqueeze(1)\n",
        "\n",
        "# x_train = torch.cat((\n",
        "#     torch.linspace(-8, -5, steps=5000),\n",
        "#     torch.linspace(-5, 0, steps=60000),\n",
        "#     torch.linspace(0, 12, steps=35000)\n",
        "# )).unsqueeze(1)\n",
        "\n",
        "y_train = swish(x_train)  # Target value\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "\n",
        "epochs = 20000\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()  # Reset gradients\n",
        "    y_pred = model(x_train)  # Forward pass, including v_reg and z_reg calculations\n",
        "    loss = criterion(y_pred, y_train)  # Remove `.squeeze()` call\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "       print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "       loss_values.append(loss.item())\n",
        "\n",
        "# Print final loss\n",
        "print(\"Loss values:\", loss_values)\n",
        "print(f'Final Loss: {loss.item()}')\n",
        "print(f'Final h: {model.h.data}')\n",
        "print(f'Final d: {model.d.data}')\n",
        "print(f'Final T: {model.T.data}')\n",
        "\n",
        "# Calculate model predictions\n",
        "with torch.no_grad():  # No gradient computation needed\n",
        "    y_pred = model(x_train).squeeze()\n",
        "\n",
        "# True function values\n",
        "y_true = swish(x_train).squeeze()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_train.numpy(), y_true.numpy(), label='True Swish Function', color='r')\n",
        "plt.plot(x_train.numpy(), y_pred.numpy(), label='Model Prediction', linestyle='--', color='b')\n",
        "plt.title('Comparison between True Swish Function and Model Prediction')\n",
        "plt.xlabel('Input x')\n",
        "plt.ylabel('Output y')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
